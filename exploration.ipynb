{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c74c215-7aee-4825-8593-07f63399406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\rodri\\Documents\\projects\\iris-classification-ml\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7539aeb1-6908-472a-a638-aaac3d6e3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"iris-classification-404414\"\n",
    "BUCKET = \"iris-classification-bucket\"\n",
    "REGION = \"southamerica-east1\"\n",
    "OUTDIR = f\"gs://{BUCKET}/iris/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bedaf61-bbe4-487c-9d23-a45db5dbdfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=iris-classification-404414\n",
      "env: BUCKET=iris-classification-bucket\n",
      "env: REGION=southamerica-east1\n",
      "env: OUTDIR=gs://iris-classification-bucket/iris/data\n",
      "env: TFVERSION=2.14\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION\n",
    "%env OUTDIR=$OUTDIR\n",
    "%env TFVERSION=2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Your active project does not match the quota project in your local Application Default Credentials file. This might result in unexpected quota issues.\n",
      "\n",
      "To update your Application Default Credentials quota project, use the `gcloud auth application-default set-quota-project` command.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "iris = fetch_ucirepo(id=53)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = iris.data.features\n",
    "y = iris.data.targets\n",
    "_, y_train = np.unique(y, return_inverse=True)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "result = pd.concat([X, y], axis=1)\n",
    "\n",
    "train, eval = train_test_split(result, test_size=0.2)\n",
    "result.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "bucket.blob('input/train.csv').upload_from_string(train.to_csv(index=False), 'text/csv')\n",
    "bucket.blob('input/eval.csv').upload_from_string(eval.to_csv(index=False), 'text/csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c59b9a6c-bc47-47ed-a7c0-7ab801368f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [versï¿½o 10.0.22621.2283]\n",
      "(c) Microsoft Corporation. Todos os direitos reservados.\n",
      "\n",
      "(venv) C:\\Users\\rodri\\Documents\\projects\\aisr>gsutil ls gs://$BUCKET/taxifare/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BadRequestException: 400 Invalid bucket name: '$BUCKET'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(venv) C:\\Users\\rodri\\Documents\\projects\\aisr>"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python ./iris/setup.py sdist --formats=gztar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64a38c3-0a17-4074-8c8a-bc8f9d13cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 15:35:54.130646: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-03 15:35:54.131027: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-03 15:35:54.139637: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-03 15:35:54.651952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " pickup_longitude (InputLay  [(None, 1)]                  0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " dropoff_longitude (InputLa  [(None, 1)]                  0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " pickup_latitude (InputLaye  [(None, 1)]                  0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " dropoff_latitude (InputLay  [(None, 1)]                  0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " scale_plon (Lambda)         (None, 1)                    0         ['pickup_longitude[0][0]']    \n",
      "                                                                                                  \n",
      " scale_dlon (Lambda)         (None, 1)                    0         ['dropoff_longitude[0][0]']   \n",
      "                                                                                                  \n",
      " scale_plat (Lambda)         (None, 1)                    0         ['pickup_latitude[0][0]']     \n",
      "                                                                                                  \n",
      " scale_dlat (Lambda)         (None, 1)                    0         ['dropoff_latitude[0][0]']    \n",
      "                                                                                                  \n",
      " plon_bkt (Discretization)   (None, 1)                    0         ['scale_plon[0][0]']          \n",
      "                                                                                                  \n",
      " plat_bkt (Discretization)   (None, 1)                    0         ['scale_plat[0][0]']          \n",
      "                                                                                                  \n",
      " dlon_bkt (Discretization)   (None, 1)                    0         ['scale_dlon[0][0]']          \n",
      "                                                                                                  \n",
      " dlat_bkt (Discretization)   (None, 1)                    0         ['scale_dlat[0][0]']          \n",
      "                                                                                                  \n",
      " p_fc (HashedCrossing)       (None, 1)                    0         ['plon_bkt[0][0]',            \n",
      "                                                                     'plat_bkt[0][0]']            \n",
      "                                                                                                  \n",
      " d_fc (HashedCrossing)       (None, 1)                    0         ['dlon_bkt[0][0]',            \n",
      "                                                                     'dlat_bkt[0][0]']            \n",
      "                                                                                                  \n",
      " pd_fc (HashedCrossing)      (None, 1)                    0         ['p_fc[0][0]',                \n",
      "                                                                     'd_fc[0][0]']                \n",
      "                                                                                                  \n",
      " pd_embed (Embedding)        (None, 1, 10)                100000    ['pd_fc[0][0]']               \n",
      "                                                                                                  \n",
      " euclidean (Lambda)          (None, 1)                    0         ['pickup_longitude[0][0]',    \n",
      "                                                                     'pickup_latitude[0][0]',     \n",
      "                                                                     'dropoff_longitude[0][0]',   \n",
      "                                                                     'dropoff_latitude[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 10)                   0         ['pd_embed[0][0]']            \n",
      "                                                                                                  \n",
      " passenger_count (InputLaye  [(None, 1)]                  0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 16)                   0         ['scale_plon[0][0]',          \n",
      "                                                                     'scale_dlon[0][0]',          \n",
      "                                                                     'scale_plat[0][0]',          \n",
      "                                                                     'scale_dlat[0][0]',          \n",
      "                                                                     'euclidean[0][0]',           \n",
      "                                                                     'flatten[0][0]',             \n",
      "                                                                     'passenger_count[0][0]']     \n",
      "                                                                                                  \n",
      " h0 (Dense)                  (None, 32)                   544       ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " h1 (Dense)                  (None, 8)                    264       ['h0[0][0]']                  \n",
      "                                                                                                  \n",
      " fare (Dense)                (None, 1)                    9         ['h1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 100817 (393.82 KB)\n",
      "Trainable params: 100817 (393.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/taxifare/trainer/task.py\", line 59, in <module>\n",
      "    model.train_and_evaluate(hparams)\n",
      "  File \"/home/jupyter/taxifare/trainer/model.py\", line 200, in train_and_evaluate\n",
      "    trainds = create_train_dataset(train_data_path, batch_size)\n",
      "  File \"/home/jupyter/taxifare/trainer/model.py\", line 61, in create_train_dataset\n",
      "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
      "  File \"/home/jupyter/taxifare/trainer/model.py\", line 49, in load_dataset\n",
      "    dataset = tf.data.experimental.make_csv_dataset(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py\", line 487, in make_csv_dataset_v2\n",
      "    filenames = _get_file_names(file_pattern, False)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/readers.py\", line 1148, in _get_file_names\n",
      "    raise ValueError(f\"No files match `file_pattern` {file_pattern}.\")\n",
      "ValueError: No files match `file_pattern` ../data/taxi-traffic-train*.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'EVAL_DATA_PATH=../data/taxi-traffic-valid*\\nTRAIN_DATA_PATH=../data/taxi-traffic-train*\\nOUTPUT_DIR=./taxifare-model\\n\\ntest ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\\n    \\n# Run the trainer module package locally with 1 eval\\n\\npython3 -m trainer.task \\\\\\n--eval_data_path $EVAL_DATA_PATH \\\\\\n--output_dir $OUTPUT_DIR \\\\\\n--train_data_path $TRAIN_DATA_PATH \\\\\\n--batch_size 5 \\\\\\n--num_examples_to_train_on 100 \\\\\\n--num_evals 1 \\\\\\n--nbuckets 10 \\\\\\n--lr 0.001 \\\\\\n--nnsize \"32 8\"\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_cell_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbash\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mEVAL_DATA_PATH=../data/taxi-traffic-valid*\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mTRAIN_DATA_PATH=../data/taxi-traffic-train*\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mOUTPUT_DIR=./taxifare-model\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mtest $\u001B[39;49m\u001B[38;5;132;43;01m{OUTPUT_DIR}\u001B[39;49;00m\u001B[38;5;124;43m && rm -rf $\u001B[39;49m\u001B[38;5;132;43;01m{OUTPUT_DIR}\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mexport PYTHONPATH=$\u001B[39;49m\u001B[38;5;132;43;01m{PYTHONPATH}\u001B[39;49;00m\u001B[38;5;124;43m:$\u001B[39;49m\u001B[38;5;132;43;01m{PWD}\u001B[39;49;00m\u001B[38;5;124;43m/taxifare\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m    \u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m# Run the trainer module package locally with 1 eval\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mpython3 -m trainer.task \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--eval_data_path $EVAL_DATA_PATH \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--output_dir $OUTPUT_DIR \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--train_data_path $TRAIN_DATA_PATH \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--batch_size 5 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--num_examples_to_train_on 100 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--num_evals 1 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--nbuckets 10 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--lr 0.001 \u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m--nnsize \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m32 8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2478\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2476\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2477\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2478\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2481\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2482\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001B[0m, in \u001B[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001B[0;34m(line, cell)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m     line \u001B[38;5;241m=\u001B[39m script\n\u001B[0;32m--> 154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshebang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001B[0m, in \u001B[0;36mScriptMagics.shebang\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mraise_error \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    310\u001B[0m     \u001B[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001B[39;00m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001B[39;00m\n\u001B[1;32m    313\u001B[0m     rc \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(rc, cell)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command 'b'EVAL_DATA_PATH=../data/taxi-traffic-valid*\\nTRAIN_DATA_PATH=../data/taxi-traffic-train*\\nOUTPUT_DIR=./taxifare-model\\n\\ntest ${OUTPUT_DIR} && rm -rf ${OUTPUT_DIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\\n    \\n# Run the trainer module package locally with 1 eval\\n\\npython3 -m trainer.task \\\\\\n--eval_data_path $EVAL_DATA_PATH \\\\\\n--output_dir $OUTPUT_DIR \\\\\\n--train_data_path $TRAIN_DATA_PATH \\\\\\n--batch_size 5 \\\\\\n--num_examples_to_train_on 100 \\\\\\n--num_evals 1 \\\\\\n--nbuckets 10 \\\\\\n--lr 0.001 \\\\\\n--nnsize \"32 8\"\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "!python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--lr 0.001 \\"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
